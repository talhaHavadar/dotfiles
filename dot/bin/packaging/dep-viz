#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "requests",
#     "graphviz",
#     "python-debian",
# ]
# ///
"""Big thanks to https://github.com/hkhonming for sharing this tool with me.

This script can/may be changed to fit my needs. Please use with caution.
"""
import argparse
import requests
import gzip
import lzma
import sys
from graphviz import Digraph
from debian.deb822 import Sources, Packages, PkgRelation


def fetch_repo_file(base_url, dist, path):
    """Fetches and decompresses a repository file (Packages or Sources)."""
    extensions = ["xz", "gz"]
    # Ensure URL ends with /
    if not base_url.endswith("/"):
        base_url += "/"

    for ext in extensions:
        url = f"{base_url}{path}.{ext}"
        try:
            print(f"[-] Attempting to fetch: {url}")
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                print(f"[+] Successfully fetched {url}")
                if ext == "xz":
                    return lzma.decompress(response.content).decode("utf-8")
                else:
                    return gzip.decompress(response.content).decode("utf-8")
        except Exception as e:
            print(f"[!] Connection error: {e}")
            continue

    print(f"[!] Error: Could not find or download file at {base_url}{path}")
    sys.exit(1)


def fetch_packages_file(base_url, dist, arch):
    """Fetches and decompresses the Packages file from the PPA."""
    base_path = f"dists/{dist}/main/binary-{arch}/Packages"
    return fetch_repo_file(base_url, dist, base_path)


def fetch_sources_file(base_url, dist):
    """Fetches and decompresses the Sources file from the PPA."""
    base_path = f"dists/{dist}/main/source/Sources"
    return fetch_repo_file(base_url, dist, base_path)


def parse_packages(content):
    """Parses the Packages file content using python-debian."""
    return list(Packages.iter_paragraphs(content))


def parse_sources(content):
    """Parses the Sources file content using python-debian."""
    return list(Sources.iter_paragraphs(content))


def get_source_name(pkg_info):
    """Extracts the Source package name."""
    if "Source" in pkg_info:
        return pkg_info["Source"].split(" ")[0].strip()
    return pkg_info["Package"]


def parse_dependency_names(dep_field):
    """Parses a dependency field and returns a list of package names.

    Uses python-debian's PkgRelation to properly handle:
    - Version constraints: (>= 1.0)
    - Architecture qualifiers: [amd64 arm64]
    - Build profiles: <!nocheck>, <!pkg.foo.bar>
    - Alternatives: pkg1 | pkg2 (returns first alternative)
    - Architecture qualifiers on package: pkg:any, pkg:native

    Args:
        dep_field: Raw dependency string from Depends/Build-Depends field

    Returns:
        List of package names (strings)
    """
    if not dep_field:
        return []

    try:
        relations = PkgRelation.parse_relations(dep_field)
    except Exception:
        # Fallback to empty if parsing fails
        return []

    pkg_names = []
    for relation in relations:
        # Each relation is a list of alternatives, pick the first one
        if relation:
            first_alt = relation[0]
            pkg_name = first_alt.get("name", "")
            if pkg_name:
                # Remove architecture qualifier suffix (:any, :native, etc.)
                if ":" in pkg_name:
                    pkg_name = pkg_name.split(":")[0]
                pkg_names.append(pkg_name)

    return pkg_names


def build_binary_graph(packages):
    """Generates a graph based on Binary Package dependencies."""
    dot = Digraph(comment="PPA Binary Dependency Chart")
    dot.attr(rankdir="LR")
    dot.attr("node", shape="box", style="filled", fillcolor="lightblue")

    # Map of all binaries available in this PPA
    ppa_binaries = {p["Package"] for p in packages}

    # Track added nodes to avoid duplicates
    added_nodes = set()

    for pkg in packages:
        pkg_name = pkg["Package"]

        if pkg_name not in added_nodes:
            dot.node(pkg_name, pkg_name)
            added_nodes.add(pkg_name)

        depends_raw = pkg.get("Depends", "")
        dep_names = parse_dependency_names(depends_raw)

        for target_bin in dep_names:
            # ONLY draw if target is also in the PPA
            if target_bin in ppa_binaries:
                dot.edge(pkg_name, target_bin)

    return dot


def build_source_graph(sources):
    """Generates a graph based on Source Package Build-Depends.

    Returns:
        tuple: (Digraph, set of edges, set of source names)
    """
    dot = Digraph(comment="PPA Source Dependency Chart")
    dot.attr(rankdir="LR")
    dot.attr("node", shape="component", style="filled", fillcolor="#E0E0E0")

    # Map Binary -> Source (from the Binary field in Sources)
    bin_to_source_map = {}
    ppa_sources = set()

    for src in sources:
        src_name = src["Package"]
        ppa_sources.add(src_name)

        # The Binary field lists all binary packages built from this source
        binaries = src.get("Binary", "")
        for bin_name in binaries.split(","):
            bin_name = bin_name.strip()
            if bin_name:
                bin_to_source_map[bin_name] = src_name

    edges = set()

    for src in sources:
        src_from = src["Package"]
        build_depends_raw = src.get("Build-Depends", "")
        dep_names = parse_dependency_names(build_depends_raw)

        for bin_target in dep_names:
            # Resolve target binary to target source
            # If bin_target is not in map, it's external (system) -> Skip
            if bin_target in bin_to_source_map:
                src_to = bin_to_source_map[bin_target]

                if src_from != src_to:
                    edges.add((src_from, src_to))

    for src in ppa_sources:
        dot.node(src, src)

    for src_from, src_to in edges:
        dot.edge(src_from, src_to)

    return dot, edges, ppa_sources


def find_strongly_connected_components(sources, edges):
    """Finds strongly connected components using Tarjan's algorithm.

    Returns:
        list of sets: Each set contains package names in one SCC
    """
    # Build adjacency list
    graph = {src: set() for src in sources}
    for src_from, src_to in edges:
        graph[src_from].add(src_to)

    index_counter = [0]
    stack = []
    lowlinks = {}
    index = {}
    on_stack = {}
    sccs = []

    def strongconnect(node):
        index[node] = index_counter[0]
        lowlinks[node] = index_counter[0]
        index_counter[0] += 1
        stack.append(node)
        on_stack[node] = True

        for successor in graph[node]:
            if successor not in index:
                strongconnect(successor)
                lowlinks[node] = min(lowlinks[node], lowlinks[successor])
            elif on_stack.get(successor, False):
                lowlinks[node] = min(lowlinks[node], index[successor])

        if lowlinks[node] == index[node]:
            scc = set()
            while True:
                successor = stack.pop()
                on_stack[successor] = False
                scc.add(successor)
                if successor == node:
                    break
            sccs.append(scc)

    for node in sources:
        if node not in index:
            strongconnect(node)

    return sccs


def compute_dependency_groups(sources, edges):
    """Computes dependency groups (topological levels) for source packages.

    Group 1: Packages with no dependencies within the PPA
    Group 2: Packages that only depend on Group 1 packages
    Group N: Packages that only depend on packages in groups < N

    Handles cyclic dependencies by detecting SCCs and marking them appropriately.

    Returns:
        tuple: (groups, cycles_info)
            - groups: list of dicts with 'packages' and 'is_cycle' keys
            - cycles_info: dict mapping cycle packages to their cycle members
    """
    # Find strongly connected components
    sccs = find_strongly_connected_components(sources, edges)

    # Identify cycles (SCCs with more than one node)
    cycles = [scc for scc in sccs if len(scc) > 1]
    cycle_packages = set()
    cycles_info = {}
    for cycle in cycles:
        for pkg in cycle:
            cycle_packages.add(pkg)
            cycles_info[pkg] = cycle

    if cycles:
        cycle_names = [', '.join(sorted(c)) for c in cycles]
        print(f"[!] Warning: Circular dependencies detected:")
        for i, cycle in enumerate(cycles, 1):
            print(f"    Cycle {i}: {', '.join(sorted(cycle))}")

    # Build adjacency list: src -> set of dependencies
    dependencies = {src: set() for src in sources}
    for src_from, src_to in edges:
        dependencies[src_from].add(src_to)

    # Create a mapping from package to its SCC representative
    pkg_to_scc = {}
    scc_to_pkgs = {}
    for i, scc in enumerate(sccs):
        rep = f"__scc_{i}__"
        scc_to_pkgs[rep] = scc
        for pkg in scc:
            pkg_to_scc[pkg] = rep

    # Build contracted graph (SCCs as nodes)
    contracted_deps = {f"__scc_{i}__": set() for i in range(len(sccs))}
    for src_from, src_to in edges:
        rep_from = pkg_to_scc[src_from]
        rep_to = pkg_to_scc[src_to]
        if rep_from != rep_to:
            contracted_deps[rep_from].add(rep_to)

    # Topological sort on contracted graph
    groups = []
    assigned = set()
    scc_reps = set(contracted_deps.keys())

    while len(assigned) < len(scc_reps):
        current_group_reps = []
        for rep in scc_reps:
            if rep in assigned:
                continue
            if contracted_deps[rep].issubset(assigned):
                current_group_reps.append(rep)

        if not current_group_reps:
            # This shouldn't happen after SCC contraction, but handle it
            remaining = [rep for rep in scc_reps if rep not in assigned]
            for rep in remaining:
                pkgs = scc_to_pkgs[rep]
                is_cycle = len(pkgs) > 1
                groups.append({'packages': sorted(pkgs), 'is_cycle': is_cycle, 'cycles': []})
            break

        # Collect all packages at this level, tracking which are cyclic
        level_packages = []
        level_cycles = []  # List of cycle sets at this level

        for rep in current_group_reps:
            pkgs = scc_to_pkgs[rep]
            if len(pkgs) > 1:
                # Cyclic SCC - sort by internal dependency count
                pkg_dep_count = {}
                for pkg in pkgs:
                    internal_deps = dependencies[pkg] & pkgs
                    pkg_dep_count[pkg] = len(internal_deps)
                sorted_pkgs = sorted(pkgs, key=lambda p: (pkg_dep_count[p], p))
                level_packages.extend(sorted_pkgs)
                level_cycles.append(pkgs)
            else:
                level_packages.extend(pkgs)

        # Sort non-cyclic packages alphabetically, keep cyclic packages in their order
        cyclic_pkg_set = set()
        for cycle in level_cycles:
            cyclic_pkg_set.update(cycle)

        non_cyclic = sorted([p for p in level_packages if p not in cyclic_pkg_set])
        cyclic_ordered = [p for p in level_packages if p in cyclic_pkg_set]

        # Combine: non-cyclic first, then cyclic
        final_packages = non_cyclic + cyclic_ordered

        groups.append({
            'packages': final_packages,
            'is_cycle': bool(level_cycles),
            'cycles': level_cycles
        })

        assigned.update(current_group_reps)

    return groups, cycles_info


def print_dependency_groups(groups, cycles_info, edges):
    """Prints dependency groups in a formatted way.

    Args:
        groups: list of dicts with 'packages', 'is_cycle', and 'cycles' keys
        cycles_info: dict mapping cycle packages to their cycle members
        edges: set of (from, to) tuples representing dependencies
    """
    # Build dependency lookup
    deps_map = {}
    for src_from, src_to in edges:
        if src_from not in deps_map:
            deps_map[src_from] = set()
        deps_map[src_from].add(src_to)

    print("\n" + "=" * 60)
    print("BUILD ORDER (Dependency Groups)")
    print("=" * 60)
    print("Packages in the same group can be built in parallel.")
    print("Groups must be built in order (Group 1 first, then Group 2, etc.)")
    print("-" * 60)

    group_num = 1
    for group_info in groups:
        packages = group_info['packages']
        cycles = group_info.get('cycles', [])

        # Build set of all cyclic packages in this group
        cyclic_pkgs = set()
        for cycle in cycles:
            cyclic_pkgs.update(cycle)

        has_cycles = bool(cyclic_pkgs)
        non_cyclic_pkgs = [p for p in packages if p not in cyclic_pkgs]

        if has_cycles:
            print(f"\nGroup {group_num} ({len(packages)} packages) [contains CYCLIC dependencies]:")

            # Show non-cyclic packages first
            if non_cyclic_pkgs:
                print(f"  Regular packages:")
                print(f"    {', '.join(non_cyclic_pkgs)}")

            # Show cyclic packages with their dependencies
            print(f"  Cyclic packages (require bootstrapping):")
            for pkg in packages:
                if pkg in cyclic_pkgs:
                    cycle_deps = deps_map.get(pkg, set()) & cyclic_pkgs
                    if cycle_deps:
                        print(f"    {pkg} -> depends on: {', '.join(sorted(cycle_deps))}")
                    else:
                        print(f"    {pkg}")
        else:
            print(f"\nGroup {group_num} ({len(packages)} package{'s' if len(packages) > 1 else ''}):")
            print(f"  {', '.join(packages)}")

        group_num += 1

    # Print summary if there were cycles
    if cycles_info:
        print("\n" + "-" * 60)
        print("CYCLE RESOLUTION NOTES:")
        print("  Packages marked as cyclic require bootstrapping:")
        print("  1. Build with reduced features or stub dependencies")
        print("  2. Rebuild with full dependencies after initial pass")

    print("\n" + "=" * 60)


def build_reverse_depends_binary(packages, target_pkg):
    """Finds all binary packages that depend on the target package.

    Args:
        packages: List of parsed package paragraphs
        target_pkg: Name of the package to find reverse dependencies for

    Returns:
        dict: Mapping of package name -> list of dependency types (e.g., 'Depends', 'Recommends')
    """
    # Map of all binaries available in this PPA
    ppa_binaries = {p["Package"] for p in packages}

    if target_pkg not in ppa_binaries:
        print(f"[!] Warning: Package '{target_pkg}' not found in repository")

    reverse_deps = {}
    dep_fields = ["Depends", "Pre-Depends", "Recommends", "Suggests"]

    for pkg in packages:
        pkg_name = pkg["Package"]
        if pkg_name == target_pkg:
            continue

        for field in dep_fields:
            dep_raw = pkg.get(field, "")
            dep_names = parse_dependency_names(dep_raw)

            if target_pkg in dep_names:
                if pkg_name not in reverse_deps:
                    reverse_deps[pkg_name] = []
                reverse_deps[pkg_name].append(field)

    return reverse_deps


def build_reverse_depends_source(sources, target_pkg):
    """Finds all source packages that build-depend on the target package.

    Args:
        sources: List of parsed source paragraphs
        target_pkg: Name of the package (binary or source) to find reverse build-deps for

    Returns:
        dict: Mapping of source package name -> list of dependency types
    """
    # Map Binary -> Source and Source -> Binaries
    bin_to_source = {}
    source_to_binaries = {}
    ppa_sources = set()

    for src in sources:
        src_name = src["Package"]
        ppa_sources.add(src_name)
        source_to_binaries[src_name] = set()

        binaries = src.get("Binary", "")
        for bin_name in binaries.split(","):
            bin_name = bin_name.strip()
            if bin_name:
                bin_to_source[bin_name] = src_name
                source_to_binaries[src_name].add(bin_name)

    # Determine the target source and all binaries to search for
    target_source = None
    target_binaries = set()

    if target_pkg in ppa_sources:
        # Target is a source package - search for all its binaries
        target_source = target_pkg
        target_binaries = source_to_binaries.get(target_pkg, set())
        # Also include the source name itself in case it's used directly
        target_binaries.add(target_pkg)
        print(f"[*] Source package '{target_pkg}' provides: {', '.join(sorted(target_binaries - {target_pkg})) or '(none)'}")
    elif target_pkg in bin_to_source:
        # Target is a binary package - just search for that binary
        target_source = bin_to_source[target_pkg]
        target_binaries.add(target_pkg)
    else:
        # Unknown package - still try to search for it
        target_binaries.add(target_pkg)
        print(f"[!] Warning: Package '{target_pkg}' not found in repository")

    reverse_deps = {}
    dep_fields = ["Build-Depends", "Build-Depends-Indep", "Build-Depends-Arch"]

    for src in sources:
        src_name = src["Package"]
        if src_name == target_source:
            continue

        for field in dep_fields:
            dep_raw = src.get(field, "")
            dep_names = set(parse_dependency_names(dep_raw))

            # Check if any of the target binaries appear in dependencies
            matched_binaries = dep_names & target_binaries
            for matched_bin in matched_binaries:
                if src_name not in reverse_deps:
                    reverse_deps[src_name] = []
                reverse_deps[src_name].append(f"{field} ({matched_bin})")

    return reverse_deps


def build_reverse_depends_source_recursive(sources, target_pkg):
    """Recursively finds all source packages that depend on the target.

    Args:
        sources: List of parsed source paragraphs
        target_pkg: Name of the source package to start from

    Returns:
        dict: Mapping of source package -> dict with 'via' (dependency path) and 'deps' (direct dep info)
    """
    # Build mappings
    bin_to_source = {}
    source_to_binaries = {}
    ppa_sources = set()

    for src in sources:
        src_name = src["Package"]
        ppa_sources.add(src_name)
        source_to_binaries[src_name] = set()

        binaries = src.get("Binary", "")
        for bin_name in binaries.split(","):
            bin_name = bin_name.strip()
            if bin_name:
                bin_to_source[bin_name] = src_name
                source_to_binaries[src_name].add(bin_name)

    # Pre-compute all reverse dependencies for each source package
    dep_fields = ["Build-Depends", "Build-Depends-Indep", "Build-Depends-Arch"]
    # rdeps_map[source] = list of (dependent_source, field, matched_binary)
    rdeps_map = {src: [] for src in ppa_sources}

    for src in sources:
        src_name = src["Package"]

        for field in dep_fields:
            dep_raw = src.get(field, "")
            dep_names = set(parse_dependency_names(dep_raw))

            for dep_bin in dep_names:
                if dep_bin in bin_to_source:
                    dep_source = bin_to_source[dep_bin]
                    if dep_source != src_name:
                        rdeps_map[dep_source].append((src_name, field, dep_bin))

    # BFS to find all transitive reverse dependencies
    if target_pkg not in ppa_sources:
        print(f"[!] Warning: Source package '{target_pkg}' not found in repository")
        return {}, []

    print(f"[*] Source package '{target_pkg}' provides: {', '.join(sorted(source_to_binaries.get(target_pkg, set()))) or '(none)'}")

    visited = set()
    result = {}  # pkg -> {'level': int, 'via': [path], 'deps': [(field, binary)]}
    queue = [(target_pkg, 0, [target_pkg])]  # (package, level, path)
    traversal_order = []

    while queue:
        current, level, path = queue.pop(0)

        for dependent, field, matched_bin in rdeps_map.get(current, []):
            if dependent not in visited:
                visited.add(dependent)
                new_path = path + [dependent]
                result[dependent] = {
                    'level': level + 1,
                    'via': new_path,
                    'deps': [(field, matched_bin, current)]
                }
                traversal_order.append(dependent)
                queue.append((dependent, level + 1, new_path))
            elif dependent in result:
                # Add additional dependency info
                result[dependent]['deps'].append((field, matched_bin, current))

    return result, traversal_order


def build_reverse_depends_binary_recursive(packages, target_pkg):
    """Recursively finds all binary packages that depend on the target.

    Args:
        packages: List of parsed package paragraphs
        target_pkg: Name of the binary package to start from

    Returns:
        dict: Mapping of package -> dict with 'level', 'via' (path), and 'deps' info
    """
    ppa_binaries = {p["Package"] for p in packages}

    if target_pkg not in ppa_binaries:
        print(f"[!] Warning: Package '{target_pkg}' not found in repository")

    # Pre-compute reverse dependencies
    dep_fields = ["Depends", "Pre-Depends", "Recommends", "Suggests"]
    # rdeps_map[pkg] = list of (dependent_pkg, field)
    rdeps_map = {p["Package"]: [] for p in packages}

    for pkg in packages:
        pkg_name = pkg["Package"]

        for field in dep_fields:
            dep_raw = pkg.get(field, "")
            dep_names = set(parse_dependency_names(dep_raw))

            for dep_bin in dep_names:
                if dep_bin in rdeps_map and dep_bin != pkg_name:
                    rdeps_map[dep_bin].append((pkg_name, field))

    # BFS traversal
    visited = set()
    result = {}
    queue = [(target_pkg, 0, [target_pkg])]
    traversal_order = []

    while queue:
        current, level, path = queue.pop(0)

        for dependent, field in rdeps_map.get(current, []):
            if dependent not in visited:
                visited.add(dependent)
                new_path = path + [dependent]
                result[dependent] = {
                    'level': level + 1,
                    'via': new_path,
                    'deps': [(field, current)]
                }
                traversal_order.append(dependent)
                queue.append((dependent, level + 1, new_path))
            elif dependent in result:
                result[dependent]['deps'].append((field, current))

    return result, traversal_order


def print_reverse_depends(reverse_deps, target_pkg, mode):
    """Prints reverse dependencies in a formatted way."""
    print(f"\n{'=' * 60}")
    print(f"REVERSE DEPENDENCIES FOR: {target_pkg}")
    print(f"Mode: {mode}")
    print(f"{'=' * 60}")

    if not reverse_deps:
        print(f"\nNo packages in this repository depend on '{target_pkg}'")
    else:
        print(f"\n{len(reverse_deps)} package(s) depend on '{target_pkg}':\n")

        # Sort by package name
        for pkg_name in sorted(reverse_deps.keys()):
            dep_types = reverse_deps[pkg_name]
            print(f"  {pkg_name}")
            for dep_type in dep_types:
                print(f"    - {dep_type}")

    print(f"\n{'=' * 60}")


def print_reverse_depends_recursive(result, traversal_order, target_pkg, mode):
    """Prints recursive reverse dependencies in a tree-like format."""
    print(f"\n{'=' * 60}")
    print(f"RECURSIVE REVERSE DEPENDENCIES FOR: {target_pkg}")
    print(f"Mode: {mode}")
    print(f"{'=' * 60}")

    if not result:
        print(f"\nNo packages in this repository depend on '{target_pkg}'")
        print(f"\n{'=' * 60}")
        return

    # Group by level
    levels = {}
    for pkg, info in result.items():
        level = info['level']
        if level not in levels:
            levels[level] = []
        levels[level].append(pkg)

    print(f"\n{len(result)} package(s) depend on '{target_pkg}' (directly or transitively):\n")

    # Print by level
    for level in sorted(levels.keys()):
        pkgs = sorted(levels[level])
        print(f"Level {level} ({len(pkgs)} package{'s' if len(pkgs) > 1 else ''}):")

        for pkg in pkgs:
            info = result[pkg]
            # Show the dependency path
            path_str = " -> ".join(info['via'])
            print(f"  {pkg}")
            print(f"    path: {path_str}")

            # Show what it depends on
            for dep_info in info['deps']:
                if len(dep_info) == 3:  # source mode: (field, binary, source)
                    field, binary, source = dep_info
                    print(f"    - {field} ({binary}) from {source}")
                else:  # binary mode: (field, pkg)
                    field, dep_pkg = dep_info
                    print(f"    - {field} on {dep_pkg}")
        print()

    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description="Generate PPA Dependency Charts")

    # Required Args
    parser.add_argument(
        "--ppa",
        required=True,
        help="The base URL of the PPA (e.g., https://ppa.../ubuntu/)",
    )

    # Optional Args with Defaults
    parser.add_argument(
        "--dist", default="noble", help="Distribution codename (default: noble)"
    )
    parser.add_argument(
        "--arch", default="amd64",
        help="Architecture for binary mode (default: amd64). "
             "Only applies to --mode=binary. Common values: amd64, arm64, i386"
    )
    parser.add_argument(
        "--output",
        default="dependency_graph",
        help="Output filename (without extension)",
    )
    parser.add_argument(
        "--mode",
        choices=["binary", "source"],
        default="binary",
        help="Graph mode: 'binary' (package-to-package) or 'source' (aggregated by source)",
    )
    parser.add_argument(
        "--rdepends",
        metavar="PACKAGE",
        help="Show reverse dependencies for the specified package (like reverse-depends)",
    )
    parser.add_argument(
        "-X", "--recursive",
        action="store_true",
        help="Recursively follow reverse dependencies (only valid with --rdepends)",
    )

    args = parser.parse_args()

    # Validate -X is only used with --rdepends
    if args.recursive and not args.rdepends:
        parser.error("-X/--recursive requires --rdepends")

    # Warn if --arch is specified with source mode (it's ignored)
    if args.mode == "source" and "--arch" in sys.argv:
        print("[!] Note: --arch is ignored in source mode (Sources file is architecture-independent)")

    # 1. Fetch and Parse
    if args.mode == "binary":
        print(f"[*] Fetching Packages metadata for {args.dist}/{args.arch}...")
        content = fetch_packages_file(args.ppa, args.dist, args.arch)
        packages = parse_packages(content)
        print(f"[*] Found {len(packages)} binary packages.")

        # Handle reverse depends query
        if args.rdepends:
            mode_str = f"binary ({args.arch})"
            if args.recursive:
                result, order = build_reverse_depends_binary_recursive(packages, args.rdepends)
                print_reverse_depends_recursive(result, order, args.rdepends, mode_str)
            else:
                reverse_deps = build_reverse_depends_binary(packages, args.rdepends)
                print_reverse_depends(reverse_deps, args.rdepends, mode_str)
            return

        # 2. Build Graph
        print("[*] Generating BINARY level dependency graph...")
        dot = build_binary_graph(packages)
    else:
        print(f"[*] Fetching Sources metadata for {args.dist}...")
        content = fetch_sources_file(args.ppa, args.dist)
        sources = parse_sources(content)
        print(f"[*] Found {len(sources)} source packages.")

        # Handle reverse depends query
        if args.rdepends:
            if args.recursive:
                result, order = build_reverse_depends_source_recursive(sources, args.rdepends)
                print_reverse_depends_recursive(result, order, args.rdepends, "source (Build-Depends)")
            else:
                reverse_deps = build_reverse_depends_source(sources, args.rdepends)
                print_reverse_depends(reverse_deps, args.rdepends, "source (Build-Depends)")
            return

        # 2. Build Graph
        print("[*] Generating SOURCE level dependency graph (using Build-Depends)...")
        dot, edges, ppa_sources = build_source_graph(sources)

        # 3. Compute and print dependency groups
        groups, cycles_info = compute_dependency_groups(ppa_sources, edges)
        print_dependency_groups(groups, cycles_info, edges)

    # 4. Render
    try:
        output_path = dot.render(args.output, format="png", cleanup=True)
        print(f"[SUCCESS] Chart saved to: {output_path}")
    except Exception as e:
        print(f"[!] Error rendering graph: {e}")
        print(
            "    Ensure 'graphviz' is installed on your system (e.g., 'sudo apt install graphviz')"
        )


if __name__ == "__main__":
    main()
